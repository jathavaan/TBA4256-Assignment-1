{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing external packages\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing internal packages\n",
    "from src.enums import Files\n",
    "from src.features.point_cloud_handler import Handler\n",
    "from src.features.pre_process import Reduce\n",
    "from src.features.display import Visualize, Plot\n",
    "from src.features.cluster import DensityBasedClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Creating a point cloud object from a las/laz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10434804 points from off_ground_points.las\n"
     ]
    }
   ],
   "source": [
    "cloud_compare_point_cloud: o3d.geometry.PointCloud = Handler.open(\n",
    "    Files.OFF_GROUND_POINTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### CFS and SOR\n",
    "The first step in the preprocessing was using the cloth simulation filter (CFS) in Cloud Compare. This is done to separate the ground- and off-ground points. We are only interested in the off-ground points and will from now on only work with these points.\n",
    "After this a statistical outlier removal (SOR) was done to remove the remaining noise. The result from this can be seen in the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.display(point_cloud=cloud_compare_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel downsampling\n",
    "The next step was to downsample the point cloud using voxel downsampling. This was done to reduce the number of points in the point cloud and to make the point cloud more uniform. The point cloud is also easier to work with due to the number of points are reduced. If is important to tune the paramaters in such a way that the semantic of the point cloud is maintaned and that there is enough points to be able to extract features from the point cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original point count: 10434804\n",
      "Downsampled point count: 9647943\n",
      "Point cloud size reduced with 7.54%\n"
     ]
    }
   ],
   "source": [
    "downsampled_point_cloud: o3d.geometry.PointCloud = Reduce.voxel_downsample(cloud_compare_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=PointCloud with 9647943 points..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jatha/Code/NTNU/TBA4256-3D-Digital-Modelling/TBA4256-Assignment-1/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dbscan: \u001b[39m'\u001b[39m\u001b[39mDensityBasedClustering\u001b[39m\u001b[39m'\u001b[39m \u001b[39m=\u001b[39m DensityBasedClustering()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jatha/Code/NTNU/TBA4256-3D-Digital-Modelling/TBA4256-Assignment-1/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dbscan\u001b[39m.\u001b[39;49mcluster(downsampled_point_cloud)\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\src\\features\\cluster\\density_based_clustering.py:36\u001b[0m, in \u001b[0;36mDensityBasedClustering.cluster\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcluster\u001b[39m(\u001b[39mself\u001b[39m, X: pd\u001b[39m.\u001b[39mDataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     dbscan: DBSCAN \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbscan_object\n\u001b[1;32m---> 36\u001b[0m     labels: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m dbscan\u001b[39m.\u001b[39;49mfit_predict(X\u001b[39m=\u001b[39;49mX)\n\u001b[0;32m     38\u001b[0m     cluster_count: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(labels)\u001b[39m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\venv\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:454\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    430\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \n\u001b[0;32m    432\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\venv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\venv\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:376\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39m@_fit_context\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[39m# DBSCAN.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    352\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform DBSCAN clustering from features, or distance matrix.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39m        Returns a fitted instance of self.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    378\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m         sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\venv\\lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\jatha\\Code\\NTNU\\TBA4256-3D-Digital-Modelling\\TBA4256-Assignment-1\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:930\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    928\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    931\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    936\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=PointCloud with 9647943 points..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "dbscan: 'DensityBasedClustering' = DensityBasedClustering()\n",
    "dbscan.cluster(downsampled_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
